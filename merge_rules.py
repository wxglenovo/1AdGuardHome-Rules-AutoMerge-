#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import requests

URLS_FILE = "urls.txt"
TMP_DIR = "tmp"
DIST_DIR = "dist"
MERGED_FILE = os.path.join(DIST_DIR, "merged_rules.txt")
LOG_FILE = os.path.join(DIST_DIR, "log.txt")

os.makedirs(TMP_DIR, exist_ok=True)
os.makedirs(DIST_DIR, exist_ok=True)

def process_line(line):
    line = line.strip()
    log_msgs = []
    results = []

    if not line:
        return results, log_msgs

    # æ³¨é‡Šè¡Œ
    if line.startswith("!"):
        log_msgs.append(f"ğŸš« å»æ‰æ³¨é‡Šè¡Œ: {line}")
        return results, log_msgs

    # HOSTS è§„åˆ™è½¬æ¢
    if line.startswith("0.0.0.0") or line.startswith("127.0.0.1"):
        parts = line.split()
        if len(parts) >= 2:
            domain = parts[1]
            new_rule = f"|{domain}^"
            results.append(new_rule)
            log_msgs.append(f"âœ… HOSTS è½¬æ¢: {line} â†’ {new_rule}")
        return results, log_msgs

    # å¤šåŸŸåæ‹†åˆ†é€»è¾‘ï¼ˆåŸè§„åˆ™åªæ‰“å°ä¸€æ¬¡ï¼Œæ‹†åˆ†è§„åˆ™é€è¡Œæ‰“å°ï¼‰
    sep = ''
    if '##' in line:
        sep = '##'
    elif '#@#' in line:
        sep = '#@#'
    elif '#?#' in line:
        sep = '#?#'

    if sep and ',' in line.split(sep)[0]:
        domains_part, suffix = line.split(sep, 1)
        # åˆ¤æ–­å‰ç¼€
        prefix = ''
        if domains_part.startswith('||'):
            prefix = '||'
            domains_part = domains_part[2:]
        elif domains_part.startswith('|'):
            prefix = '|'
            domains_part = domains_part[1:]
        else:
            prefix = '|'
        domains = [d.strip() for d in domains_part.split(',')]
        new_rules = [f"{prefix}{d}{sep}{suffix}" for d in domains]
        results.extend(new_rules)
        # æ—¥å¿—ï¼šåŸè§„åˆ™ä¸€æ¬¡ï¼Œæ‹†åˆ†åé€æ¡æ‰“å°
        log_msgs.append(f"âœ… å¤šåŸŸåæ‹†åˆ†: {line}")
        for r in new_rules:
            log_msgs.append(f"    â†’ {r}")
        return results, log_msgs

    # æ™®é€šè§„åˆ™ï¼Œä¸æ‰“å°æ—¥å¿—
    results.append(line)
    return results, log_msgs

merged_rules = []
log_lines = []

if not os.path.exists(URLS_FILE):
    print(f"âš  {URLS_FILE} ä¸å­˜åœ¨")
    exit(1)

with open(URLS_FILE, 'r', encoding='utf-8') as f:
    urls = [line.strip() for line in f if line.strip()]

for idx, url in enumerate(urls, start=1):
    print(f"ğŸ”— å¼€å§‹å¤„ç†ç¬¬ {idx}/{len(urls)} ä¸ªæº: {url}")
    try:
        r = requests.get(url, timeout=20)
        r.raise_for_status()
        lines = r.text.splitlines()
        processed = []
        for line in lines:
            results, logs = process_line(line)
            for log in logs:
                print(log)          # é€æ¡æ‰“å°æ—¥å¿—
                log_lines.append(log)
            processed.extend(results)
        # ä¿å­˜æ¯ä¸ªæºæ‹†åˆ†åçš„è§„åˆ™
        tmp_file = os.path.join(TMP_DIR, f"{idx:03}.txt")
        with open(tmp_file, 'w', encoding='utf-8') as ftmp:
            ftmp.write('\n'.join(processed))
        merged_rules.extend(processed)
    except Exception as e:
        print(f"âŒ ä¸‹è½½æˆ–å¤„ç†å¤±è´¥: {e}")

# ä¿å­˜åˆå¹¶åçš„è§„åˆ™
with open(MERGED_FILE, 'w', encoding='utf-8') as f:
    f.write('\n'.join(merged_rules))

# ä¿å­˜æ—¥å¿—
with open(LOG_FILE, 'w', encoding='utf-8') as f:
    f.write('\n'.join(log_lines))

print(f"ğŸ‰ å®Œæˆï¼å…±ç”Ÿæˆ {len(merged_rules)} æ¡è§„åˆ™")
print(f"ğŸ“‚ tmp/ æ–‡ä»¶: {len(os.listdir(TMP_DIR))} ä¸ª")
print(f"ğŸ“„ åˆå¹¶è§„åˆ™æ–‡ä»¶: {MERGED_FILE}")
print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {LOG_FILE}")
